# Import necessary libraries
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.naive_bayes import GaussianNB
from scipy.stats import zscore, norm, ttest_ind
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the Iris dataset and explore its structure with df.head(), df.info(), and df.describe().
iris = load_iris(as_frame=True)
df = iris.frame

# Explore the dataset
print("First few rows of the dataset:")
print(df.head())
print("\nDataset Info:")
print(df.info())
print("\nStatistical Summary:")
print(df.describe())

# 2. Apply encodings with pd.get_dummies() and transformations.
# Encode the target variable
df_encoded = pd.get_dummies(df, columns=['target'], drop_first=True)

# Scale numerical features
scaler = StandardScaler()
df_transformed = df.copy()
df_transformed[iris.feature_names] = scaler.fit_transform(df_transformed[iris.feature_names])

# 3. Build a linear regression model with Scikit-learn and evaluate it with R² and MSE.
X = df[iris.feature_names]
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred = lr_model.predict(X_test)

print("\nLinear Regression R² Score:", r2_score(y_test, y_pred))
print("Linear Regression MSE:", mean_squared_error(y_test, y_pred))

# 4. Create visualizations with Matplotlib and Seaborn.
sns.pairplot(df, hue='target')
plt.show()

sns.histplot(df['sepal length (cm)'], kde=True)
plt.show()

# 5. Clean data by removing outliers and handling duplicate rows.
df_cleaned = df[(zscore(df[iris.feature_names]).abs() < 3).all(axis=1)]
df_cleaned = df_cleaned.drop_duplicates()

# 6. Apply k-NN using Scikit-learn on the Iris dataset.
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
print("\nk-NN Accuracy:", accuracy_score(y_test, y_pred_knn))

# 7. Visualize correlations with Seaborn’s heatmap and pairplot.
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()

sns.pairplot(df, hue='target')
plt.show()

# 8. Plot and analyze distributions using Scipy and Seaborn.
sns.kdeplot(df['sepal width (cm)'], label='KDE')
sns.lineplot(x=df['sepal width (cm)'], 
             y=norm.pdf(df['sepal width (cm)'], loc=df['sepal width (cm)'].mean(), scale=df['sepal width (cm)'].std()), 
             label='Normal')
plt.legend()
plt.show()

# 9. Apply k-Means clustering on a dataset and visualize clusters.
kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(X)
sns.scatterplot(x=df['sepal length (cm)'], y=df['sepal width (cm)'], hue=df['cluster'], palette='viridis')
plt.show()

# 10. Use df.isna() and fillna() to clean missing values.
print("\nMissing Values Before Cleaning:")
print(df.isna().sum())

df.fillna(df.mean(), inplace=True)
print("Missing Values After Cleaning:")
print(df.isna().sum())

# 11. Perform t-tests using scipy.stats.ttest_ind().
species_0 = df[df['target'] == 0]['sepal length (cm)']
species_1 = df[df['target'] == 1]['sepal length (cm)']
stat, p_val = ttest_ind(species_0, species_1)
print("\nt-test p-value between species 0 and 1:", p_val)

# 12. Train and test a Naive Bayes classifier on a dataset.
nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_test)
print("\nNaive Bayes Accuracy:", accuracy_score(y_test, y_pred_nb))
